{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b2b9b2",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cbfed6",
   "metadata": {},
   "source": [
    "## Benchmark different policies to assess impact on average reward\n",
    "In May 2020, I joined the [Machine Learning Tokyo](https://machinelearningtokyo.com/) study group on Reinforcement Learning.\n",
    "\n",
    "This online RL series covered the book \"Reinforcement learning: An introduction\" by Richard Sutton and Andrew Barto.\n",
    "One of the chapters was on the multi-armed Bandits problem.\n",
    "This is like being in front of a multi-armed slot machine\n",
    "Each action selection is like a play of one of the slot machineâ€™s levers, and the rewards are the payoffs for hitting the jackpot. \n",
    "Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers.\n",
    "\n",
    "In order to understand it properly, I decided to implement in python the pseudo codes from the book.\n",
    "The idea was to compare the impact of different policies on the average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937ae10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aca80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_rewards(n_experiments = 1000, # Set the number of experiments, default 1000\n",
    "                    steps = 10000, # Set the number of steps, default 10000\n",
    "                    k = 10, # Set the number of arms of the Multi-Armed bandit\n",
    "                    optimistic = 0, # Set optimistic action values estimates at start up\n",
    "                    epsilon = 0.1, # Set the proportion of exploration\n",
    "                    alpha = None, # Set constant step-size parameter\n",
    "                    beta = False,\n",
    "                    stationary = True \n",
    "                    ):\n",
    "    \n",
    "    averages = np.zeros([steps]) # Initialize averages\n",
    "    \n",
    "    experiments_j = 0 # Initialize experiment count\n",
    "    \n",
    "    if alpha is None:\n",
    "        for j in range(n_experiments):\n",
    "            experiments_j += 1 # Update experiment index\n",
    "            q_star = np.random.normal(0, 1, k) # Set actual action values\n",
    "            step = np.zeros([k]) # Initialize step counter\n",
    "            Q = np.zeros([k]) + optimistic # Initialize action values estimates\n",
    "            avg=[] # Initialize average\n",
    "            avg.append(0)\n",
    "            \n",
    "            for j in range(steps):\n",
    "                exploration = np.random.uniform() < epsilon # Draw between exploration and exploitation\n",
    "                index = (not exploration)*np.random.choice(np.where(Q == Q.max())[0]) + exploration*np.random.randint(0,k)\n",
    "                step[index]+=1\n",
    "                r=np.random.normal(q_star[index], 1) # Compute reward\n",
    "                Q[index]= Q[index] + (r-Q[index])/step[index] # Update action values estimates\n",
    "                r=avg[-1]+(r-avg[-1])/np.sum(step) # Update average reward\n",
    "                avg.append(r)\n",
    "                q_star=q_star+(1-stationary)*np.random.normal(0, 0.01, k)\n",
    "            avg.pop(0)       \n",
    "            averages = averages + (avg-averages)/experiments_j\n",
    "    else:\n",
    "        if not(beta):\n",
    "            for j in range(n_experiments):\n",
    "                experiments_j += 1 # Update experiment index\n",
    "                q_star = np.random.normal(0, 1, k) # Set actual action values\n",
    "                step = np.zeros([k]) # Initialize step counter\n",
    "                Q = np.zeros([k]) + optimistic # Initialize action values estimates\n",
    "                avg=[] # Initialize average\n",
    "                avg.append(0)\n",
    "                \n",
    "                for j in range(steps):\n",
    "                    exploration = np.random.uniform() < epsilon # Draw between exploration and exploitation\n",
    "                    index = (not exploration)*np.random.choice(np.where(Q == Q.max())[0]) + exploration*np.random.randint(0,k)\n",
    "                    step[index]+=1\n",
    "                    r=np.random.normal(q_star[index], 1) # Compute reward\n",
    "                    Q[index]= Q[index] + (r-Q[index])*alpha # Update action values estimates\n",
    "                    r=avg[-1]+(r-avg[-1])/np.sum(step) # Update average reward\n",
    "                    avg.append(r)\n",
    "                    q_star=q_star+(1-stationary)*np.random.normal(0, 0.01, k)\n",
    "                avg.pop(0)       \n",
    "                averages = averages + (avg-averages)/experiments_j\n",
    "        else:\n",
    "            for j in range(n_experiments):\n",
    "                experiments_j += 1 # Update experiment index\n",
    "                q_star = np.random.normal(0, 1, k) # Set actual action values\n",
    "                step = np.zeros([k]) # Initialize step counter\n",
    "                Q = np.zeros([k]) + optimistic # Initialize action values estimates\n",
    "                avg=[] # Initialize average\n",
    "                avg.append(0)\n",
    "                o=0\n",
    "                \n",
    "                for j in range(steps):\n",
    "                    exploration = np.random.uniform() < epsilon # Draw between exploration and exploitation\n",
    "                    index = (not exploration)*np.random.choice(np.where(Q == Q.max())[0]) + exploration*np.random.randint(0,k)\n",
    "                    step[index]+=1\n",
    "                    r=np.random.normal(q_star[index], 1) # Compute reward\n",
    "                    o = o + alpha*(1-o)\n",
    "                    beta_ = alpha/o\n",
    "                    Q[index]= Q[index] + (r-Q[index])*beta_ # Update action values estimates\n",
    "                    r=avg[-1]+(r-avg[-1])/np.sum(step) # Update average reward\n",
    "                    avg.append(r)\n",
    "                    q_star=q_star+(1-stationary)*np.random.normal(0, 0.01, k)\n",
    "                avg.pop(0)       \n",
    "                averages = averages + (avg-averages)/experiments_j\n",
    "        \n",
    "    return averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e442c13",
   "metadata": {},
   "source": [
    "## Description of the policies tested:\n",
    "### Random\n",
    "Playing randomly\n",
    "\n",
    "### Greedy: \n",
    "Systematically playing the current best value action\n",
    "\n",
    "### Epsilon Greedy: \n",
    "Same as the greedy policy but epsilon fraction of the time play randomly to explore for better actions,\n",
    "\n",
    "### Optimistic Greedy: \n",
    "same as greedy policy but you start with very optimistic value estimates to explore all actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the results for several policies:\n",
    "# Random play\n",
    "averages_random = average_rewards(epsilon=1)\n",
    "\n",
    "# Greedy policy: systematically playing the current best value action, \n",
    "averages_greedy = average_rewards(epsilon=0)\n",
    "\n",
    "# Epsilon greedy policy: same as the greedy policy but epsilon fraction of the time play randomly to explore for better actions,\n",
    "averages_epsilon_greedy = average_rewards(epsilon=0.1)\n",
    "\n",
    "# Average optimisitc greedy: same as greedy policy but you start with very optimistic value estimates to explore all actions,\n",
    "averages_optimistic_greedy = average_rewards(epsilon=0, optimistic=5)\n",
    "\n",
    "# Random play non stationary\n",
    "averages_random_not_stationary = average_rewards(epsilon = 1, stationary =False)\n",
    "\n",
    "# Greedy policy: systematically playing the current best value action non stationary\n",
    "averages_greedy_not_stationary = average_rewards(epsilon = 0, stationary =False)\n",
    "\n",
    "# Epsilon greedy policy: same as the greedy policy but epsilon fraction of the time play randomly to explore for better actions, non stationary\n",
    "averages_epsilon_greedy_not_stationary = average_rewards(epsilon = 0.1, stationary =False)\n",
    "\n",
    "# Average optimisitc greedy: same as greedy policy but you start with very optimistic value estimates to explore all actions, non stationary\n",
    "averages_optimistic_greedy_not_stationary = average_rewards(epsilon = 0, optimistic = 5, stationary =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adaf1db",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(15,12));\n",
    "ax1.set_title(\"Average Reward versus Steps, Stationary Problem (linear, linear)\");  \n",
    "ax1.set_xlabel(\"Steps (linear scale)\");\n",
    "ax1.set_ylabel(\"Average Reward (linear scale)\");\n",
    "ax1.grid(visible=True, which='both');\n",
    "\n",
    "ax2.set_xscale('log');\n",
    "ax2.set_title(\"Average Reward versus Steps, Stationary Problem (x log, y linear)\");\n",
    "ax2.set_xlabel(\"Steps (log scale)\");\n",
    "ax2.set_ylabel(\"Average Reward (linear scale)\");\n",
    "ax2.grid(visible=True, which='both');\n",
    "\n",
    "ax3.set_title(\"Average Reward versus Steps, Non Stationary Problem (linear, linear)\");  \n",
    "ax3.set_xlabel(\"Steps (linear scale)\");\n",
    "ax3.set_ylabel(\"Average Reward (linear scale)\");\n",
    "ax3.grid(visible=True, which='both');\n",
    "\n",
    "ax4.set_xscale('log');\n",
    "ax4.set_title(\"Average Reward versus Steps, Non Stationary Problem (x log, y linear)\");\n",
    "ax4.set_xlabel(\"Steps (log scale)\");\n",
    "ax4.set_ylabel(\"Average Reward (linear scale)\");\n",
    "ax4.grid(visible=True, which='both');\n",
    "\n",
    "\n",
    "ax1.plot(range(1,len(averages_random)+1),averages_random, label='Random');\n",
    "ax1.plot(range(1,len(averages_greedy)+1),averages_greedy, label='Greedy');\n",
    "ax1.plot(range(1,len(averages_epsilon_greedy)+1),averages_epsilon_greedy, label='Epsilon Greedy: 0.1');\n",
    "ax1.plot(range(1,len(averages_optimistic_greedy)+1),averages_optimistic_greedy, label='Optimistic Greedy');\n",
    "ax1.axhline(1.55, c=\"black\", label='Optimal')\n",
    "\n",
    "ax2.plot(range(1,len(averages_random)+1),averages_random, label='Random');\n",
    "ax2.plot(range(1,len(averages_greedy)+1),averages_greedy, label='Greedy');\n",
    "ax2.plot(range(1,len(averages_epsilon_greedy)+1),averages_epsilon_greedy, label='Epsilon Greedy: 0.1');\n",
    "ax2.plot(range(1,len(averages_optimistic_greedy)+1),averages_optimistic_greedy, label='Optimistic Greedy');\n",
    "ax2.axhline(1.55, c=\"black\", label='Optimal')\n",
    "\n",
    "ax3.plot(range(1,len(averages_random_not_stationary)+1),averages_random_not_stationary, label='Random');\n",
    "ax3.plot(range(1,len(averages_greedy_not_stationary)+1),averages_greedy_not_stationary, label='Greedy');\n",
    "ax3.plot(range(1,len(averages_epsilon_greedy_not_stationary)+1),averages_epsilon_greedy_not_stationary, label='Epsilon Greedy: 0.1');\n",
    "ax3.plot(range(1,len(averages_optimistic_greedy_not_stationary)+1),averages_optimistic_greedy_not_stationary, label='Optimistic Greedy');\n",
    "ax3.axhline(1.55, c=\"black\", label='Optimal')\n",
    "\n",
    "ax4.plot(range(1,len(averages_random_not_stationary)+1),averages_random_not_stationary, label='Random');\n",
    "ax4.plot(range(1,len(averages_greedy_not_stationary)+1),averages_greedy_not_stationary, label='Greedy');\n",
    "ax4.plot(range(1,len(averages_epsilon_greedy_not_stationary)+1),averages_epsilon_greedy_not_stationary, label='Epsilon Greedy: 0.1');\n",
    "ax4.plot(range(1,len(averages_optimistic_greedy_not_stationary)+1),averages_optimistic_greedy_not_stationary, label='Optimistic Greedy');\n",
    "ax4.axhline(1.55, c=\"black\", label='Optimal')\n",
    "\n",
    "ax1.legend();\n",
    "ax2.legend();\n",
    "ax3.legend();\n",
    "ax4.legend();\n",
    "\n",
    "ax1.set_ylim(bottom=-0.25, top=1.75);\n",
    "ax2.set_ylim(bottom=-0.25, top=1.75);\n",
    "ax3.set_ylim(bottom=-0.25, top=1.75);\n",
    "ax4.set_ylim(bottom=-0.25, top=1.75);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8932b3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Conclusions and Way Forward\n",
    "Among the policies tested, the optimistic greedy gave the best results on the stationary problem.\n",
    "\n",
    "The next steps would be to test thoses policies on non stationary problems and on problems where the state changes as a result of the actions."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
